# ğŸš€ Module 2 - Talk Mode Quick Start

## âœ… Module 2 is Complete!

Samy can now have voice conversations! Here's what to do next.

---

## ğŸ”§ Setup (First Time Only)

### 1. Install New Dependencies

```bash
cd apps/api
pnpm install
```

This installs:
- `openai` - For Whisper & GPT
- `multer` - For audio uploads
- `axios` & `form-data` - For ElevenLabs API

### 2. Add API Keys to `.env`

Make sure your root `.env` has:

```env
OPENAI_API_KEY=sk-your-key-here
ELEVENLABS_API_KEY=your-key-here
```

**Get ElevenLabs Key:** https://elevenlabs.io/app/settings/api-keys

---

## ğŸ® Run & Test

### 1. Start Both Servers

```bash
# Terminal 1 - API Server
pnpm --filter @apps/api dev

# Terminal 2 - Web App
pnpm --filter @apps/web dev
```

### 2. Test in Browser

1. Open http://localhost:3000
2. Click "Start Listening"
3. Scroll to "ğŸ™ï¸ Talk to Samy"
4. Click "ğŸ¤ Hold to Talk"
5. Speak a message (e.g., "Hello Samy!")
6. Click to stop recording
7. Watch Samy respond with voice! ğŸ‰

### 3. Test API Endpoints (Optional)

```bash
# Run automated tests
node scripts/test-talk-endpoints.js

# Or test manually
curl -X POST http://localhost:3001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"sessionId":"demo-session","message":"Hello!"}'
```

---

## ğŸ“š Documentation

- **`MODULE2_COMPLETE.md`** - Full feature summary
- **`MODULE2_TESTING.md`** - Detailed testing guide with cURL examples
- **`scripts/test-talk-endpoints.js`** - Automated test script

---

## ğŸ¯ What's New

### 3 New API Endpoints

1. **`POST /api/listen`** - Transcribe audio â†’ text (Whisper)
2. **`POST /api/chat`** - Get GPT response with emotion
3. **`POST /api/speak`** - Convert text â†’ speech (ElevenLabs)

### New Web Feature

- **"Talk to Samy"** section with:
  - Voice recording from browser
  - Real-time transcript display
  - GPT reply shown
  - Auto-play of speech response
  - Beautiful UI with status indicators

### Firestore Integration

- State updates automatically during conversation:
  - `listening` â†’ while recording
  - `thinking` â†’ while GPT processes
  - `speaking` â†’ while playing audio
  - `idle` â†’ after conversation

---

## ğŸ” Quick Troubleshooting

| Issue | Solution |
|-------|----------|
| "OPENAI_API_KEY not configured" | Add key to root `.env` |
| "ELEVENLABS_API_KEY not configured" | Add key to root `.env` and restart server |
| "Microphone error" | Allow microphone access in browser |
| No sound playing | Check audio URL in browser console |
| API not responding | Restart API server |

---

## ğŸ‰ Next: Module 3

With voice working, you can now add:
- ğŸ­ 3D Avatar with Three.js
- ğŸ˜Š Emotion-based facial expressions
- ğŸ‘„ Lip-sync animation
- ğŸ¨ Gestures and idle animations

---

## ğŸ“ Key Files Modified

```
apps/api/src/
â”œâ”€â”€ services/          # NEW
â”‚   â”œâ”€â”€ whisper.ts    # Speech-to-text
â”‚   â”œâ”€â”€ gpt.ts        # Chat with emotion
â”‚   â””â”€â”€ tts.ts        # Text-to-speech
â”œâ”€â”€ app.ts            # Added 3 endpoints
â””â”€â”€ package.json      # Added dependencies

apps/web/app/
â””â”€â”€ page.tsx          # Added Talk UI

scripts/
â””â”€â”€ test-talk-endpoints.js  # NEW - Test script
```

---

**That's it! Module 2 is ready to use.** ğŸ¤ğŸ»

Try talking to Samy now at http://localhost:3000!

